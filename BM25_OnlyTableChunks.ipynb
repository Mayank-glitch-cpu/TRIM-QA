{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "69047f02-21f0-4ceb-ab2c-6da7cd9b7eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/itewari1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/itewari1/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Necessary Resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f786b09e-6b7b-4c40-b43b-5289945feabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution parameters\n",
    "file_path = \"tables.jsonl\"\n",
    "dev_file_path = \"test.jsonl\"\n",
    "output_dir = \"results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2aeec853-9160-4fe0-b15c-337cf9a5b3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized corpus loaded.\n"
     ]
    }
   ],
   "source": [
    "#Load the table tokens\n",
    "with open('tokenized_table_corpus.pkl', 'rb') as f:\n",
    "    tokenized_chunks_from_file = pickle.load(f)\n",
    "\n",
    "print(\"Tokenized corpus loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc96db5a-8b2c-49ff-8675-2bb799c64904",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################ Chunking #################################\n",
    "\n",
    "# Row chunking and its metadata\n",
    "def chunk_row(row, row_id, table_name, columns):\n",
    "    row_text = ' | '.join([f\"{columns[i]['text']}: {cell['text']}\" for i, cell in enumerate(row['cells']) if columns[i]['text']])\n",
    "    return {\n",
    "        \"text\": row_text,\n",
    "        \"metadata\": {\n",
    "            \"table_name\": table_name,\n",
    "            \"row_id\": row_id,\n",
    "            \"chunk_id\": f\"{table_name}_row_{row_id}\",\n",
    "            \"chunk_type\": \"row\",\n",
    "            \"columns\": [col[\"text\"] for col in columns],\n",
    "            \"metadata_text\": f\"table: {table_name}, row: {row_id}, chunk_id: {table_name}_row_{row_id}, chunk_type: row, columns: {', '.join([col['text'] for col in columns if col['text']])}\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Column chunk and its metadata\n",
    "def chunk_column(rows, col_id, col_name, table_name):\n",
    "    column_text = ' | '.join([row['cells'][col_id]['text'] for row in rows if row['cells'][col_id]['text']])\n",
    "\n",
    "    return {\n",
    "        \"text\": f\"{col_name if col_name else ''}: {column_text}\",\n",
    "        \"metadata\": {\n",
    "            \"table_name\": table_name,\n",
    "            \"col_id\": col_id,\n",
    "            \"chunk_id\": f\"{table_name}_column_{col_id}\",\n",
    "            \"chunk_type\": \"column\",\n",
    "            \"metadata_text\": f\"table: {table_name}, col: {col_name if col_name else ''}, chunk_id: {table_name}_column_{col_id}, chunk_type: column\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Table chunking with its metadata\n",
    "def chunk_table(rows, table_id, columns):\n",
    "    column_names = \" | \".join([col['text'] for col in columns])\n",
    "    table_text = '\\n'.join([column_names] + [' | '.join([cell['text'] for cell in row['cells']]) for row in rows])\n",
    "\n",
    "    return {\n",
    "        \"text\": table_text,\n",
    "        \"metadata\": {\n",
    "            \"table_name\": table_id,\n",
    "            \"chunk_id\": f\"{table_id}_table\",\n",
    "            \"chunk_type\": \"table\",\n",
    "            \"columns\": [col[\"text\"] for col in columns],  # Adding column names\n",
    "            \"metadata_text\": f\"table_name: {table_id}, chunk_id: {table_id}_table, chunk_type: table, columns: {', '.join([col['text'] for col in columns])}\"\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4bdc620-9d35-43e3-9ac4-13031dc9f29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## Processing ##################################\n",
    "\n",
    "# Process jsonl file: chunking\n",
    "def process_jsonl(file_path):\n",
    "\n",
    "    metadata_list = []\n",
    "    chunks = []\n",
    "    chunk_embeddings = []\n",
    "    table_chunks = []\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in tqdm(f):\n",
    "            data = json.loads(line.strip())\n",
    "            table_id = data['tableId']\n",
    "            rows = data['rows']\n",
    "            columns = data['columns']\n",
    "\n",
    "            # Chunking row\n",
    "            for row_id, row in enumerate(rows):\n",
    "                row_chunk = chunk_row(row, row_id, table_id, columns)\n",
    "                chunks.append(row_chunk)\n",
    "                metadata_list.append(row_chunk[\"metadata\"])\n",
    "\n",
    "            # Chunking Column\n",
    "            for col_id, col in enumerate(columns):\n",
    "                if col[\"text\"]:\n",
    "                    col_chunk = chunk_column(rows, col_id, col[\"text\"], table_id)\n",
    "                    chunks.append(col_chunk)\n",
    "                    metadata_list.append(col_chunk[\"metadata\"])\n",
    "\n",
    "            # Chunking table\n",
    "            table_chunk = chunk_table(rows, table_id, columns)\n",
    "            chunks.append(table_chunk)\n",
    "            table_chunks.append(table_chunk)\n",
    "\n",
    "    return metadata_list, chunks, table_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f154a3f-4cb7-4182-a32f-a5acb964e183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank Chunks\n",
    "def rank_chunks_with_bm25(tokenized_chunks, query, top_n):\n",
    "    # Using BM25\n",
    "    bm25 = BM25Okapi([chunk['tokenized_text'] for chunk in tokenized_chunks])\n",
    "    scores = bm25.get_scores(query)\n",
    "\n",
    "    # Sort chunks by BM25 score in descending order\n",
    "    ranked_chunks = sorted(zip(scores, tokenized_chunks), reverse=True, key=lambda x: x[0])\n",
    "\n",
    "    # Get top N chunks\n",
    "    top_ranked_chunks = ranked_chunks[:top_n]\n",
    "    \n",
    "    return top_ranked_chunks\n",
    "\n",
    "# Save the top N chunks to a file\n",
    "def save_top_chunks(top_chunks, output_dir, output_filename):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(top_chunks, f, indent=2)\n",
    "\n",
    "    print(f\"Saved top chunks to {output_path}\")\n",
    "\n",
    "# Calculate recall, rank, and 10%/20% check\n",
    "def calculate_recall(ranked_chunks, correct_table_id, top_n):\n",
    "    rank = None\n",
    "    for idx, (_, chunk) in enumerate(ranked_chunks):\n",
    "        if chunk['table_id'] == correct_table_id:\n",
    "            rank = idx + 1\n",
    "\n",
    "            # Check if it's in the top 10% or 20%\n",
    "            is_in_top_10 = 1 if rank <= top_n * 0.1 else 0\n",
    "            is_in_top_20 = 1 if rank <= top_n * 0.2 else 0\n",
    "            return 1, rank, is_in_top_10, is_in_top_20\n",
    "\n",
    "    return 0, None, 0, 0  # Relevant item not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c2abed52-b57f-4952-b64b-40ed0e3b28e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getTableID():\n",
    "#     with open('results/top_chunks.json', 'r') as top_chunks_file:\n",
    "#         for i, line in enumerate(tqdm(top_chunks_file)):\n",
    "#             data = json.loads(line.strip())\n",
    "#             print(data[1]['table_id'])\n",
    "\n",
    "# getTableID()\n",
    "\n",
    " \n",
    "\n",
    "# def getTableID(ranked_chunks):\n",
    "#     table_id = []\n",
    "    # with open('results/top_chunks.json', 'r') as f:\n",
    "        #     all_data = json.load(f)  # load entire JSON array\n",
    "    \n",
    "        # for i, item in enumerate(tqdm(all_data)):\n",
    "        #     try:\n",
    "        #         table_id.append(item[1]['table_id'])\n",
    "        #         # print(table_id)\n",
    "        #     except (IndexError, KeyError, TypeError) as e:\n",
    "        #         print(f\"Skipping item {i}: {e}\")\n",
    "    \n",
    "        # # print(table_id)\n",
    "\n",
    "#     for ranked_chunk in ranked_chunks:\n",
    "#         table_id.append(ranked_chunk[1]['table_id'])\n",
    "        \n",
    "#     return table_id\n",
    "\n",
    "\n",
    "def getTableID(ranked_chunks):\n",
    "    table_ids = []\n",
    "    for chunk in ranked_chunks:\n",
    "        try:\n",
    "            table_ids.append(chunk[1]['table_id'])\n",
    "        except (IndexError, KeyError, TypeError) as e:\n",
    "            print(f\"Skipping malformed chunk: {e}\")\n",
    "    return table_ids\n",
    "\n",
    "# getTableID()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2491fe53-c3a1-4df3-94b8-0ddf55bb1783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main script\n",
    "def main():\n",
    "    # Process jsonl file and run BM25 on each queries_count\n",
    "    with open(dev_file_path, 'r') as dev_file:\n",
    "        for i, line in enumerate(tqdm(dev_file)):\n",
    "            if i >= queries_count:  \n",
    "                break\n",
    "            \n",
    "            data = json.loads(line.strip())\n",
    "            query = data['questions'][0]['originalText']\n",
    "            correct_table_id = data['table']['tableId']\n",
    "\n",
    "            tokenized_query = tokenize(query)\n",
    "            ranked_chunks = rank_chunks_with_bm25(tokenized_chunks_from_file, tokenized_query, top_n)\n",
    "\n",
    "            # print(ranked_chunks)\n",
    "\n",
    "            save_top_chunks(ranked_chunks, output_dir, \"top_chunks.json\")\n",
    "\n",
    "            # time.sleep(60)\n",
    "\n",
    "            #Put these TableIDs in a csv\n",
    "\n",
    "            output_file = f\"query{i}_TopTables.csv\"\n",
    "            table_id = getTableID(ranked_chunks)\n",
    "\n",
    "            # print(table_id)\n",
    "\n",
    "            with open(output_file, 'w', newline='') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "            \n",
    "                # Write header\n",
    "                writer.writerow(['query', 'table_id'])\n",
    "                \n",
    "            \n",
    "                # Write each (query, table_id) pair\n",
    "                for tid in table_id:\n",
    "                    writer.writerow([query, tid])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a26f5903-2db5-480f-a36c-360a9313d0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Generate Chunks\n",
    "# metadata, chunks, table_chunks = process_jsonl(file_path)\n",
    "# table_chunks = sorted(table_chunks, key=lambda x: x[\"metadata\"][\"table_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e52509e8-68e9-47a0-9214-8eedcdeaa652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Generate Tokens\n",
    "# tokenized_chunks = []\n",
    "\n",
    "# for i, chunk in enumerate(tqdm(table_chunks, desc=\"Tokenizing Chunks\", unit=\"chunk\")):\n",
    "#     table_id = chunk['metadata']['table_name']\n",
    "#     tokenized_text = tokenize(chunk['text'] + str(chunk['metadata']))\n",
    "\n",
    "#     tokenized_chunks.append({\n",
    "#         \"table_id\": table_id,\n",
    "#         \"tokenized_text\": tokenized_text,\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9ccf3a3c-6e29-4918-a034-14104e4b0627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the tokenized corpus to a pickle file\n",
    "# with open('tokenized_table_corpus.pkl', 'wb') as f:\n",
    "#     pickle.dump(tokenized_chunks, f)\n",
    "\n",
    "# print(\"Tokenized corpus saved to 'tokenized_table_corpus.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d89f6de9-26e1-40f3-90e6-b0f81469874d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:17, 17.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved top chunks to results/top_chunks.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "top_n = 5000\n",
    "queries_count = 1\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba05554-5c54-40bf-93d2-640beefd0b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59306ef3-4bc7-463a-bd5b-6c7b270744dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
