{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69047f02-21f0-4ceb-ab2c-6da7cd9b7eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/itewari1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/itewari1/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Necessary Resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f786b09e-6b7b-4c40-b43b-5289945feabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution parameters\n",
    "file_path = \"tables.jsonl\"\n",
    "dev_file_path = \"test.jsonl\"\n",
    "output_dir = \"results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc96db5a-8b2c-49ff-8675-2bb799c64904",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################ Chunking #################################\n",
    "\n",
    "# Row chunking and its metadata\n",
    "def chunk_row(row, row_id, table_name, columns):\n",
    "    row_text = ' | '.join([f\"{columns[i]['text']}: {cell['text']}\" for i, cell in enumerate(row['cells']) if columns[i]['text']])\n",
    "    return {\n",
    "        \"text\": row_text,\n",
    "        \"metadata\": {\n",
    "            \"table_name\": table_name,\n",
    "            \"row_id\": row_id,\n",
    "            \"chunk_id\": f\"{table_name}_row_{row_id}\",\n",
    "            \"chunk_type\": \"row\",\n",
    "            \"columns\": [col[\"text\"] for col in columns],\n",
    "            \"metadata_text\": f\"table: {table_name}, row: {row_id}, chunk_id: {table_name}_row_{row_id}, chunk_type: row, columns: {', '.join([col['text'] for col in columns if col['text']])}\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Column chunk and its metadata\n",
    "def chunk_column(rows, col_id, col_name, table_name):\n",
    "    column_text = ' | '.join([row['cells'][col_id]['text'] for row in rows if row['cells'][col_id]['text']])\n",
    "\n",
    "    return {\n",
    "        \"text\": f\"{col_name if col_name else ''}: {column_text}\",\n",
    "        \"metadata\": {\n",
    "            \"table_name\": table_name,\n",
    "            \"col_id\": col_id,\n",
    "            \"chunk_id\": f\"{table_name}_column_{col_id}\",\n",
    "            \"chunk_type\": \"column\",\n",
    "            \"metadata_text\": f\"table: {table_name}, col: {col_name if col_name else ''}, chunk_id: {table_name}_column_{col_id}, chunk_type: column\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Table chunking with its metadata\n",
    "def chunk_table(rows, table_id, columns):\n",
    "    column_names = \" | \".join([col['text'] for col in columns])\n",
    "    table_text = '\\n'.join([column_names] + [' | '.join([cell['text'] for cell in row['cells']]) for row in rows])\n",
    "\n",
    "    return {\n",
    "        \"text\": table_text,\n",
    "        \"metadata\": {\n",
    "            \"table_name\": table_id,\n",
    "            \"chunk_id\": f\"{table_id}_table\",\n",
    "            \"chunk_type\": \"table\",\n",
    "            \"columns\": [col[\"text\"] for col in columns],  # Adding column names\n",
    "            \"metadata_text\": f\"table_name: {table_id}, chunk_id: {table_id}_table, chunk_type: table, columns: {', '.join([col['text'] for col in columns])}\"\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4bdc620-9d35-43e3-9ac4-13031dc9f29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## Processing ##################################\n",
    "\n",
    "# Process jsonl file: chunking\n",
    "def process_jsonl(file_path):\n",
    "\n",
    "    metadata_list = []\n",
    "    chunks = []\n",
    "    chunk_embeddings = []\n",
    "    table_chunks = []\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in tqdm(f):\n",
    "            data = json.loads(line.strip())\n",
    "            table_id = data['tableId']\n",
    "            rows = data['rows']\n",
    "            columns = data['columns']\n",
    "\n",
    "            # Chunking row\n",
    "            for row_id, row in enumerate(rows):\n",
    "                row_chunk = chunk_row(row, row_id, table_id, columns)\n",
    "                chunks.append(row_chunk)\n",
    "                metadata_list.append(row_chunk[\"metadata\"])\n",
    "\n",
    "            # Chunking Column\n",
    "            for col_id, col in enumerate(columns):\n",
    "                if col[\"text\"]:\n",
    "                    col_chunk = chunk_column(rows, col_id, col[\"text\"], table_id)\n",
    "                    chunks.append(col_chunk)\n",
    "                    metadata_list.append(col_chunk[\"metadata\"])\n",
    "\n",
    "            # Chunking table\n",
    "            table_chunk = chunk_table(rows, table_id, columns)\n",
    "            chunks.append(table_chunk)\n",
    "            table_chunks.append(table_chunk)\n",
    "\n",
    "    return metadata_list, chunks, table_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a26f5903-2db5-480f-a36c-360a9313d0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Chunks\n",
    "def generate_chunks():\n",
    "    metadata, chunks, table_chunks = process_jsonl(file_path)\n",
    "    table_chunks = sorted(table_chunks, key=lambda x: x[\"metadata\"][\"table_name\"])\n",
    "    return table_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a4fdcd8-05fa-420e-a01d-aa47ac88f3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return word_tokenize(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e52509e8-68e9-47a0-9214-8eedcdeaa652",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Tokens\n",
    "def generate_token(table_chunks):\n",
    "    tokenized_chunks = []\n",
    "    \n",
    "    for i, chunk in enumerate(tqdm(table_chunks, desc=\"Tokenizing Chunks\", unit=\"chunk\")):\n",
    "        table_id = chunk['metadata']['table_name']\n",
    "        tokenized_text = tokenize(chunk['text'] + str(chunk['metadata']))\n",
    "    \n",
    "        tokenized_chunks.append({\n",
    "            \"table_id\": table_id,\n",
    "            \"tokenized_text\": tokenized_text,\n",
    "        })\n",
    "\n",
    "    # Save the tokenized corpus to a pickle file\n",
    "    with open('tokenized_table_corpus.pkl', 'wb') as f:\n",
    "        pickle.dump(tokenized_chunks, f)\n",
    "    \n",
    "    print(\"Tokenized corpus saved to 'tokenized_table_corpus.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2aeec853-9160-4fe0-b15c-337cf9a5b3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the table tokens\n",
    "def load_token(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        tokenized_chunks_from_file = pickle.load(f)\n",
    "    \n",
    "    print(\"Tokenized corpus loaded.\")\n",
    "    return tokenized_chunks_from_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f154a3f-4cb7-4182-a32f-a5acb964e183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank Chunks\n",
    "def rank_chunks_with_bm25(tokenized_chunks, query, top_n):\n",
    "    # Using BM25\n",
    "    bm25 = BM25Okapi([chunk['tokenized_text'] for chunk in tokenized_chunks])\n",
    "    scores = bm25.get_scores(query)\n",
    "\n",
    "    # Sort chunks by BM25 score in descending order\n",
    "    ranked_chunks = sorted(zip(scores, tokenized_chunks), reverse=True, key=lambda x: x[0])\n",
    "\n",
    "    # Get top N chunks\n",
    "    top_ranked_chunks = ranked_chunks[:top_n]\n",
    "    \n",
    "    return top_ranked_chunks\n",
    "\n",
    "# Save the top N chunks to a file\n",
    "def save_top_chunks(top_chunks, output_dir, output_filename):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(top_chunks, f, indent=2)\n",
    "\n",
    "    print(f\"Saved top chunks to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2abed52-b57f-4952-b64b-40ed0e3b28e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the Table IDs for the top 5000 tables \n",
    "def getTableID(ranked_chunks):\n",
    "    table_ids = []\n",
    "    for chunk in ranked_chunks:\n",
    "        try:\n",
    "            table_ids.append(chunk[1]['table_id'])\n",
    "        except (IndexError, KeyError, TypeError) as e:\n",
    "            print(f\"Skipping malformed chunk: {e}\")\n",
    "    return table_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2491fe53-c3a1-4df3-94b8-0ddf55bb1783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main script\n",
    "def main(tokenized_chunks_from_file, query_details):\n",
    "    i = 1\n",
    "    pruning_test_data_dir = 'pruningTestData'\n",
    "    for q_dict in query_details:\n",
    "        \n",
    "        query = q_dict['Queries']\n",
    "        target_table = q_dict['Target TableID']\n",
    "        target_answer = q_dict['Target Answers']\n",
    "        \n",
    "        \n",
    "        tokenized_query = tokenize(query)\n",
    "        ranked_chunks = rank_chunks_with_bm25(tokenized_chunks_from_file, tokenized_query, top_n)\n",
    "\n",
    "        # save_top_chunks(ranked_chunks, output_dir, \"top_chunks.json\")\n",
    "\n",
    "        #Put these TableIDs in a csv\n",
    "\n",
    "        # output_file = f\"query{i}_TopTables.csv\"\n",
    "        output_file = os.path.join(pruning_test_data_dir, f\"query{i}_TopTables.csv\")\n",
    "        i += 1\n",
    "        table_id = getTableID(ranked_chunks)\n",
    "\n",
    "        with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['query', 'top tables', 'target table', 'target answer'])  # Write header\n",
    "            for tid in table_id:\n",
    "                writer.writerow([query, tid, target_table, target_answer])\n",
    "\n",
    "        print(output_file, \" is ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d242e68f-d558-4e7c-be39-e034be5997c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized corpus loaded.\n"
     ]
    }
   ],
   "source": [
    "# table_chunks = generate_chunks() #Uncomment and run this if you don't have pickle file of tokens\n",
    "# generate_token(table_chunks)     #Uncomment and run this if you don't have pickle file of tokens\n",
    "tokenized_chunks_from_file = load_token('tokenized_table_corpus.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c419828-b6fc-46dd-bc37-48dd50211b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the query list\n",
    "def load_all_rows_from_csv(csv_file_path):\n",
    "    rows = []\n",
    "    with open(csv_file_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            rows.append(row)  # Each row is a dict with keys: Queries, TableID, AnswerTexts\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d89f6de9-26e1-40f3-90e6-b0f81469874d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pruningTestData/query1_TopTables.csv  is ready\n",
      "pruningTestData/query2_TopTables.csv  is ready\n",
      "pruningTestData/query3_TopTables.csv  is ready\n",
      "pruningTestData/query4_TopTables.csv  is ready\n",
      "pruningTestData/query5_TopTables.csv  is ready\n",
      "pruningTestData/query6_TopTables.csv  is ready\n",
      "pruningTestData/query7_TopTables.csv  is ready\n",
      "pruningTestData/query8_TopTables.csv  is ready\n",
      "pruningTestData/query9_TopTables.csv  is ready\n",
      "pruningTestData/query10_TopTables.csv  is ready\n",
      "pruningTestData/query11_TopTables.csv  is ready\n",
      "pruningTestData/query12_TopTables.csv  is ready\n",
      "pruningTestData/query13_TopTables.csv  is ready\n",
      "pruningTestData/query14_TopTables.csv  is ready\n",
      "pruningTestData/query15_TopTables.csv  is ready\n",
      "pruningTestData/query16_TopTables.csv  is ready\n",
      "pruningTestData/query17_TopTables.csv  is ready\n",
      "pruningTestData/query18_TopTables.csv  is ready\n",
      "pruningTestData/query19_TopTables.csv  is ready\n",
      "pruningTestData/query20_TopTables.csv  is ready\n",
      "pruningTestData/query21_TopTables.csv  is ready\n",
      "pruningTestData/query22_TopTables.csv  is ready\n",
      "pruningTestData/query23_TopTables.csv  is ready\n",
      "pruningTestData/query24_TopTables.csv  is ready\n",
      "pruningTestData/query25_TopTables.csv  is ready\n",
      "pruningTestData/query26_TopTables.csv  is ready\n",
      "pruningTestData/query27_TopTables.csv  is ready\n",
      "pruningTestData/query28_TopTables.csv  is ready\n",
      "pruningTestData/query29_TopTables.csv  is ready\n",
      "pruningTestData/query30_TopTables.csv  is ready\n",
      "pruningTestData/query31_TopTables.csv  is ready\n",
      "pruningTestData/query32_TopTables.csv  is ready\n",
      "pruningTestData/query33_TopTables.csv  is ready\n",
      "pruningTestData/query34_TopTables.csv  is ready\n",
      "pruningTestData/query35_TopTables.csv  is ready\n",
      "pruningTestData/query36_TopTables.csv  is ready\n",
      "pruningTestData/query37_TopTables.csv  is ready\n",
      "pruningTestData/query38_TopTables.csv  is ready\n",
      "pruningTestData/query39_TopTables.csv  is ready\n",
      "pruningTestData/query40_TopTables.csv  is ready\n",
      "pruningTestData/query41_TopTables.csv  is ready\n",
      "pruningTestData/query42_TopTables.csv  is ready\n",
      "pruningTestData/query43_TopTables.csv  is ready\n",
      "pruningTestData/query44_TopTables.csv  is ready\n",
      "pruningTestData/query45_TopTables.csv  is ready\n",
      "pruningTestData/query46_TopTables.csv  is ready\n",
      "pruningTestData/query47_TopTables.csv  is ready\n",
      "pruningTestData/query48_TopTables.csv  is ready\n",
      "pruningTestData/query49_TopTables.csv  is ready\n",
      "pruningTestData/query50_TopTables.csv  is ready\n",
      "pruningTestData/query51_TopTables.csv  is ready\n",
      "pruningTestData/query52_TopTables.csv  is ready\n",
      "pruningTestData/query53_TopTables.csv  is ready\n",
      "pruningTestData/query54_TopTables.csv  is ready\n",
      "pruningTestData/query55_TopTables.csv  is ready\n",
      "pruningTestData/query56_TopTables.csv  is ready\n",
      "pruningTestData/query57_TopTables.csv  is ready\n",
      "pruningTestData/query58_TopTables.csv  is ready\n",
      "pruningTestData/query59_TopTables.csv  is ready\n",
      "pruningTestData/query60_TopTables.csv  is ready\n",
      "pruningTestData/query61_TopTables.csv  is ready\n",
      "pruningTestData/query62_TopTables.csv  is ready\n",
      "pruningTestData/query63_TopTables.csv  is ready\n",
      "pruningTestData/query64_TopTables.csv  is ready\n",
      "pruningTestData/query65_TopTables.csv  is ready\n",
      "pruningTestData/query66_TopTables.csv  is ready\n",
      "pruningTestData/query67_TopTables.csv  is ready\n",
      "pruningTestData/query68_TopTables.csv  is ready\n",
      "pruningTestData/query69_TopTables.csv  is ready\n",
      "pruningTestData/query76_TopTables.csv  is ready\n",
      "pruningTestData/query77_TopTables.csv  is ready\n",
      "pruningTestData/query78_TopTables.csv  is ready\n",
      "pruningTestData/query79_TopTables.csv  is ready\n",
      "pruningTestData/query80_TopTables.csv  is ready\n",
      "pruningTestData/query81_TopTables.csv  is ready\n",
      "pruningTestData/query82_TopTables.csv  is ready\n",
      "pruningTestData/query83_TopTables.csv  is ready\n",
      "pruningTestData/query84_TopTables.csv  is ready\n",
      "pruningTestData/query85_TopTables.csv  is ready\n",
      "pruningTestData/query86_TopTables.csv  is ready\n",
      "pruningTestData/query87_TopTables.csv  is ready\n",
      "pruningTestData/query88_TopTables.csv  is ready\n",
      "pruningTestData/query89_TopTables.csv  is ready\n",
      "pruningTestData/query90_TopTables.csv  is ready\n",
      "pruningTestData/query91_TopTables.csv  is ready\n",
      "pruningTestData/query92_TopTables.csv  is ready\n",
      "pruningTestData/query93_TopTables.csv  is ready\n",
      "pruningTestData/query94_TopTables.csv  is ready\n",
      "pruningTestData/query95_TopTables.csv  is ready\n",
      "pruningTestData/query96_TopTables.csv  is ready\n",
      "pruningTestData/query97_TopTables.csv  is ready\n",
      "pruningTestData/query98_TopTables.csv  is ready\n",
      "pruningTestData/query99_TopTables.csv  is ready\n",
      "pruningTestData/query100_TopTables.csv  is ready\n",
      "pruningTestData/query101_TopTables.csv  is ready\n",
      "pruningTestData/query102_TopTables.csv  is ready\n",
      "pruningTestData/query103_TopTables.csv  is ready\n",
      "pruningTestData/query104_TopTables.csv  is ready\n",
      "pruningTestData/query105_TopTables.csv  is ready\n",
      "pruningTestData/query106_TopTables.csv  is ready\n",
      "pruningTestData/query107_TopTables.csv  is ready\n",
      "pruningTestData/query108_TopTables.csv  is ready\n",
      "pruningTestData/query109_TopTables.csv  is ready\n",
      "pruningTestData/query110_TopTables.csv  is ready\n",
      "pruningTestData/query111_TopTables.csv  is ready\n",
      "pruningTestData/query112_TopTables.csv  is ready\n",
      "pruningTestData/query113_TopTables.csv  is ready\n",
      "pruningTestData/query114_TopTables.csv  is ready\n",
      "pruningTestData/query115_TopTables.csv  is ready\n",
      "pruningTestData/query116_TopTables.csv  is ready\n",
      "pruningTestData/query117_TopTables.csv  is ready\n",
      "pruningTestData/query118_TopTables.csv  is ready\n",
      "pruningTestData/query119_TopTables.csv  is ready\n",
      "pruningTestData/query120_TopTables.csv  is ready\n",
      "pruningTestData/query121_TopTables.csv  is ready\n",
      "pruningTestData/query122_TopTables.csv  is ready\n",
      "pruningTestData/query123_TopTables.csv  is ready\n",
      "pruningTestData/query124_TopTables.csv  is ready\n",
      "pruningTestData/query125_TopTables.csv  is ready\n",
      "pruningTestData/query126_TopTables.csv  is ready\n",
      "pruningTestData/query127_TopTables.csv  is ready\n",
      "pruningTestData/query128_TopTables.csv  is ready\n",
      "pruningTestData/query129_TopTables.csv  is ready\n",
      "pruningTestData/query130_TopTables.csv  is ready\n",
      "pruningTestData/query131_TopTables.csv  is ready\n",
      "pruningTestData/query132_TopTables.csv  is ready\n",
      "pruningTestData/query133_TopTables.csv  is ready\n",
      "pruningTestData/query134_TopTables.csv  is ready\n",
      "pruningTestData/query135_TopTables.csv  is ready\n",
      "pruningTestData/query136_TopTables.csv  is ready\n",
      "pruningTestData/query137_TopTables.csv  is ready\n",
      "pruningTestData/query138_TopTables.csv  is ready\n",
      "pruningTestData/query139_TopTables.csv  is ready\n",
      "pruningTestData/query140_TopTables.csv  is ready\n",
      "pruningTestData/query141_TopTables.csv  is ready\n",
      "pruningTestData/query142_TopTables.csv  is ready\n",
      "pruningTestData/query143_TopTables.csv  is ready\n",
      "pruningTestData/query144_TopTables.csv  is ready\n",
      "pruningTestData/query145_TopTables.csv  is ready\n",
      "pruningTestData/query146_TopTables.csv  is ready\n",
      "pruningTestData/query147_TopTables.csv  is ready\n",
      "pruningTestData/query148_TopTables.csv  is ready\n",
      "pruningTestData/query149_TopTables.csv  is ready\n",
      "pruningTestData/query150_TopTables.csv  is ready\n"
     ]
    }
   ],
   "source": [
    "top_n = 5000\n",
    "# queries_count = 1\n",
    "query_details = load_all_rows_from_csv('random_queries.csv')\n",
    "# print(query_details[:2])\n",
    "main(tokenized_chunks_from_file, query_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba05554-5c54-40bf-93d2-640beefd0b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59306ef3-4bc7-463a-bd5b-6c7b270744dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
